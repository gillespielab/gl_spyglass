{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31590422",
   "metadata": {},
   "source": [
    "Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9218c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-18 13:38:31,013][INFO]: DataJoint 0.14.6 connected to gabby@gl-ash.biostr.washington.edu:3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataJoint connection (connected) gabby@gl-ash.biostr.washington.edu:3306"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datajoint as dj\n",
    "\n",
    "dj.config['database.host'] = \"gl-ash.biostr.washington.edu\"\n",
    "dj.config['database.user'] = \"gabby\"\n",
    "dj.config['database.port'] = 3306\n",
    "\n",
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f3dff",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f2a8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import spyglass.common as sgc\n",
    "import spyglass.spikesorting.v1 as sgs\n",
    "from spyglass.common.common_interval import Interval\n",
    "from spyglass.spikesorting.spikesorting_merge import SpikeSortingOutput\n",
    "from spyglass.decoding.v1.waveform_features import UnitWaveformFeatures\n",
    "\n",
    "import figpack.views as vv\n",
    "\n",
    "from gl_spyglass.utils.common_neural_functions import validate_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4100539",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['FIGPACK_BUCKET'] = 'gillespielab'\n",
    "\n",
    "# TODO: update this with your own custom FIGPACK_API_KEY (contact Jeremy Magland if you don't have one)\n",
    "os.environ['FIGPACK_API_KEY'] = '17979e3bc8880ea91520c1dc2e8ab6a97fc665893fe2728e9d642923c3dcf31c'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ecc70",
   "metadata": {},
   "source": [
    "Choose tetrodes for your relevant subj(s), date(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f265e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_sess_tetrodes = {\n",
    "    'pippin': {\n",
    "        20210421: {\n",
    "            'can1_ca1_tetrodes': [6, 9, 30],\n",
    "            'can2_ca1_tetrodes': [35, 36, 40],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d9cfa",
   "metadata": {},
   "source": [
    "Choose parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570d9c2",
   "metadata": {},
   "source": [
    "Choose between `plot_type` 'one_from_many_tet' or 'all_from_one_tet' depending on which kind of artifact detection you're visualizing. For coincicent spike detection, it's useful to look at one electrode from many tetrodes so that you can see if the cross-tetrode artifacts are getting caught. For threshold artifact detection (from `ArtifactDetection`) it's useful to look at all the electrodes from a single tetrode because it works per sort group (tetrode) to look for big artifacts that are present across multiple electrodes in the tetrode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 'pippin'\n",
    "date = 20210421\n",
    "nwb_file_name = f'{subj}{date}_.nwb'\n",
    "interval_list_name = '02_r1'\n",
    "plot_type = 'one_from_many_tet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69544313",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_info = subj_sess_tetrodes[subj][date]\n",
    "epoch = int(interval_list_name[:2])\n",
    "pos_interval_list_name = (sgc.IntervalList() & {'nwb_file_name': nwb_file_name, 'pipeline': 'position'}).fetch('interval_list_name')[epoch - 1]\n",
    "sort_interval_list_name = interval_list_name\n",
    "\n",
    "if plot_type == 'one_from_many_tet':\n",
    "    can1_ca1_tetrodes = subj_info['can1_ca1_tetrodes']\n",
    "    can2_ca1_tetrodes = subj_info['can2_ca1_tetrodes']\n",
    "\n",
    "    # get electrodes info\n",
    "    electrodes_df, val_can_refs = validate_references(nwb_file_name, is_copy=True)\n",
    "\n",
    "    # narrow down electrodes_df to good electrodes\n",
    "    electrodes_df = electrodes_df[electrodes_df['bad_channel'] == 'False']\n",
    "\n",
    "    # convert tetrode numbers to electrode group numbers \n",
    "    can1_ca1_elec_groups = np.asarray(can1_ca1_tetrodes) - 1\n",
    "    can2_ca1_elec_groups = np.asarray(can2_ca1_tetrodes) - 1\n",
    "\n",
    "    # find the electrode id for the first good channel on the tetrode\n",
    "    can1_ca1_elecs = [electrodes_df.loc[electrodes_df['electrode_group_name'] == elec_group, 'electrode_id'].values[0] for elec_group in can1_ca1_elec_groups]\n",
    "    can2_ca1_elecs = [electrodes_df.loc[electrodes_df['electrode_group_name'] == elec_group, 'electrode_id'].values[0] for elec_group in can2_ca1_elec_groups]\n",
    "\n",
    "    # select elecs and elec labels\n",
    "    plot_elecs = np.concatenate([can1_ca1_elecs, can2_ca1_elecs])\n",
    "    plot_elec_labels = np.concatenate([[f'can1 {e + 1}: elec {elec}' for e, elec in enumerate(can1_ca1_elecs)], [f'can2 {e + 1}: elec {elec}' for e, elec in enumerate(can2_ca1_elecs)]])\n",
    "\n",
    "if plot_type == 'all_from_one_tet':\n",
    "    # select which cannula and tetrode index to plot from\n",
    "    can = 1\n",
    "    tet_idx = 0\n",
    "    tetrode = subj_info[f'can{can}_ca1_tetrodes'][tet_idx]\n",
    "\n",
    "    # get electrodes info\n",
    "    electrodes_df, val_can_refs = validate_references(nwb_file_name, is_copy=True)\n",
    "\n",
    "    # narrow down electrodes_df to good electrodes\n",
    "    electrodes_df = electrodes_df[electrodes_df['bad_channel'] == 'False']\n",
    "\n",
    "    # find which electrode ids corresponds to this tetrode\n",
    "    elec_group = tetrode - 1\n",
    "    sort_group_id = (sgs.SortGroup.SortGroupElectrode() & {'nwb_file_name': nwb_file_name, 'electrode_group_name': elec_group}).fetch('sort_group_id')[0]\n",
    "    plot_elecs = electrodes_df.loc[electrodes_df['electrode_group_name'] == elec_group, 'electrode_id'].values\n",
    "    plot_elec_labels = np.asarray([f'can{can} {e + 1}: elec {elec}' for e, elec in enumerate(plot_elecs)])\n",
    "\n",
    "# set colors\n",
    "colors = ['black']*len(plot_elecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3227792",
   "metadata": {},
   "source": [
    "Load in the spike-sorted (600-6000 Hz) data for each selected electrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2b31dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cbfef846244781bbe8f5adea318140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the spike recording data from each electrode id of interest\n",
    "plot_sort_groups = [(sgs.SortGroup.SortGroupElectrode() & {'nwb_file_name': nwb_file_name, 'electrode_id': elec}).fetch1('sort_group_id') for elec in plot_elecs]\n",
    "probe_elecs = electrodes_df.loc[electrodes_df['electrode_id'].isin(plot_elecs), 'probe_electrode'].values\n",
    "elec_dict = {}\n",
    "for elec, sort_group, probe_elec in zip(plot_elecs, plot_sort_groups, probe_elecs):\n",
    "    elec_dict[elec] = [sort_group, probe_elec]\n",
    "\n",
    "for e, elec in tqdm(enumerate(elec_dict.keys())):\n",
    "    sort_group, probe_elec = elec_dict[elec]\n",
    "    recording_id = (sgs.SpikeSortingRecordingSelection() & {'nwb_file_name': nwb_file_name, 'interval_list_name': sort_interval_list_name, 'sort_group_id': sort_group}).fetch1('recording_id')\n",
    "    spike_sorting_recording = (sgs.SpikeSortingRecording & {'recording_id': recording_id}).fetch_nwb()\n",
    "    if len(spike_sorting_recording) == 0:\n",
    "        continue\n",
    "    timestamps = spike_sorting_recording[0]['object_id'].timestamps[:]\n",
    "    data = spike_sorting_recording[0]['object_id'].data[:, probe_elec]\n",
    "    if e == 0:\n",
    "        spike_recording_data = np.zeros((len(plot_elecs), data.shape[0]))\n",
    "    spike_recording_data[e, :] = data\n",
    "\n",
    "# convert to dataframe\n",
    "spike_recording_df = pd.DataFrame(spike_recording_data.T, columns=plot_elecs, index=timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7b005",
   "metadata": {},
   "source": [
    "Set plotting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = spike_recording_df\n",
    "elecs = plot_elecs \n",
    "labels = plot_elec_labels\n",
    "\n",
    "time_window = None\n",
    "data_type = 'dataframe'\n",
    "downsamp=1  # (no downsampling)\n",
    "zero_win_start = False\n",
    "hide_nav = False \n",
    "hide_time_labels = False\n",
    "sampling_frequency_hz = 30000  # make sure to update this if your data is not sampled at 30 kHz!\n",
    "elec_separation = 1000  # can adjust this if you want more or less separation between the electrodes in the plot\n",
    "plot_artifacts = True \n",
    "plot_spikes = False  # can only plot spikes for a single sort group (all_from_one_tet)\n",
    "artifact_type = 'coincident_spikes'  # 'coincident_spikes' or 'large_amplitude_events' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b52f51",
   "metadata": {},
   "source": [
    "Process the data before visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5ac95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that coincident_spikes should be used for one_from_many_tets while large_amplitude_events should be used for all_from_one_tet\n",
    "if (artifact_type == 'coincident_spikes') & (plot_type == 'all_from_one_tet'):\n",
    "    raise Warning(\"coincident_spikes artifact type is not recommended for all_from_one_tet plot type, consider switching to large_amplitude_events for better visualization of artifacts\")\n",
    "if (artifact_type == 'large_amplitude_events') & (plot_type == 'one_from_many_tets'):\n",
    "    raise Warning(\"large_amplitude_events artifact type is not recommended for one_from_many_tets plot type, consider switching to coincident_spikes for better visualization of artifacts\")\n",
    "\n",
    "if (plot_spikes) & (plot_type == 'one_from_many_tet'):\n",
    "    raise Warning(\"spikes cannot be plotted for one_from_many_tet plot type, consider switching to all_from_one_tet if you want to plot spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd332f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the data to 0 the time windows to the interval list start time\n",
    "int_start_time, int_end_time = (sgc.IntervalList() & {'nwb_file_name': nwb_file_name, 'interval_list_name': interval_list_name}).fetch1('valid_times')[0]\n",
    "int_length = int_end_time - int_start_time\n",
    "\n",
    "if time_window:\n",
    "    win_length = time_window[1] - time_window[0]\n",
    "    int_window_start = time_window[0]\n",
    "    int_window_end = time_window[1]\n",
    "else:\n",
    "    win_length = None\n",
    "    int_window_start = 0 \n",
    "    int_window_end = int_length\n",
    "\n",
    "if data_type == 'eseries':\n",
    "    win_mask = (full_data.timestamps[::downsamp] >= (int_start_time + int_window_start)) & (full_data.timestamps[::downsamp] < (int_start_time + int_window_end))    \n",
    "    timestamps = full_data.timestamps[::downsamp][win_mask]\n",
    "if data_type == 'dataframe':\n",
    "    timestamps = full_data.index.values\n",
    "    win_mask = (timestamps[::downsamp] >= (int_start_time + int_window_start)) & (timestamps[::downsamp] < (int_start_time + int_window_end))\n",
    "    timestamps = timestamps[::downsamp][win_mask]\n",
    "\n",
    "if zero_win_start:\n",
    "    win_start_time = timestamps[0]\n",
    "    timestamps -= win_start_time\n",
    "else:\n",
    "    timestamps -= int_start_time  # zero to the beginning of the interval list\n",
    "\n",
    "# initialize TimeSeriesGraph view\n",
    "view = vv.TimeseriesGraph(\n",
    "    legend_opts={\"location\": \"northeast\"},\n",
    "    hide_x_gridlines=True,\n",
    "    hide_nav_toolbar=hide_nav,\n",
    "    hide_time_axis_labels=hide_time_labels,\n",
    "    y_label='Voltage (uV)',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f262f44",
   "metadata": {},
   "source": [
    "Load in and plot artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76d8340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ArtifactDetection not yet populated for pippin20210421_.nwb, 02_r1 with artifact param name amp_3000_0.5_prop, skipping\n",
      "No artifacts were detected with artifact params pos 1 valid times coincident_spikes_removed_times rem_0.001 close_5e-05 frac_0.5\n"
     ]
    }
   ],
   "source": [
    "if plot_artifacts:\n",
    "    # Load in artifact times\n",
    "    if artifact_type == 'large_amplitude_events':\n",
    "        # plot spike artifact intervals from ArtifactDetection\n",
    "        spike_artifacts_params_name = 'amp_3000_0.5_prop'\n",
    "        recording_id = (sgs.SpikeSortingRecordingSelection() & {\n",
    "                'nwb_file_name': nwb_file_name, \n",
    "                'interval_list_name': interval_list_name, \n",
    "                'sort_group_id': sort_group_id,\n",
    "                'preproc_param_name': 'default'\n",
    "            }).fetch1('recording_id')\n",
    "        try:\n",
    "            artifact_id = str((sgs.ArtifactDetectionSelection() & {'artifact_param_name': spike_artifacts_params_name, 'recording_id': recording_id}).fetch1('artifact_id'))\n",
    "            spike_artifact_times = (sgc.IntervalList() & {'nwb_file_name': nwb_file_name, 'interval_list_name': artifact_id, 'pipeline': 'spikesorting_artifact_v1'}).fetch1('valid_times')\n",
    "        except Exception as e:\n",
    "            print(f'WARNING: ArtifactDetection not yet populated for {nwb_file_name}, {interval_list_name} with artifact param name {spike_artifacts_params_name}, skipping')\n",
    "            spike_artifact_times = []\n",
    "    \n",
    "    if artifact_type == 'coincident_spikes':\n",
    "        # plot spike artifact intervals from custom coincident spike detection pipeline\n",
    "        spike_closeness_threshold = 0.00005\n",
    "        max_coincident_fraction = 0.5\n",
    "        removal_window_s = 0.001\n",
    "        pipeline = 'coincident spike detection'\n",
    "        # spike_artifacts_params_name = f'coincident spike detection +- rem_{removal_window_s} close_{spike_closeness_threshold} frac_{max_coincident_fraction}'\n",
    "\n",
    "        epoch = int(interval_list_name[:2])\n",
    "        pos_interval_list_name = (\n",
    "            sgc.IntervalList()\n",
    "            & {\"nwb_file_name\": nwb_file_name, \"pipeline\": \"position\"}\n",
    "        ).fetch(\"interval_list_name\")[epoch - 1]\n",
    "        coinc_interval_list_name =  f'{pos_interval_list_name} coincident_spikes_removed_times rem_{removal_window_s} close_{spike_closeness_threshold} frac_{max_coincident_fraction}'\n",
    "\n",
    "        spike_valid_times = (sgc.IntervalList() & {'nwb_file_name': nwb_file_name, \n",
    "                                    'interval_list_name': coinc_interval_list_name,\n",
    "                                    # 'interval_list_name': f'{pos_interval_list_name} +- coincident_spikes_removed_times',\n",
    "                                    'pipeline': pipeline}).fetch1('valid_times')\n",
    "                                    # 'pipeline': 'coincident spike detection'}).fetch1('valid_times')\n",
    "\n",
    "        spike_artifact_times = Interval((sgc.IntervalList() & {'nwb_file_name': nwb_file_name, 'interval_list_name': pos_interval_list_name}).fetch1('valid_times')).subtract(spike_valid_times).times\n",
    "\n",
    "    # Plot the artifact intervals\n",
    "    if len(spike_artifact_times) == 0:\n",
    "        print(f'No artifacts were detected with artifact params {coinc_interval_list_name}')\n",
    "    else:\n",
    "        # convert artifact times to be on the same timescale as the data (zeroed to start of the interval list)\n",
    "        if zero_win_start:\n",
    "            spike_artifact_times = spike_artifact_times - win_start_time\n",
    "        else:\n",
    "            spike_artifact_times = spike_artifact_times - int_start_time\n",
    "\n",
    "        last_timestamp = timestamps[-1]\n",
    "        last_artifact = len(spike_artifact_times)\n",
    "        for a, (start, end) in enumerate(spike_artifact_times):\n",
    "            if end > last_timestamp:\n",
    "                last_artifact = a\n",
    "                break\n",
    "        \n",
    "        first_timestamp = timestamps[0]\n",
    "        first_artifact = 0\n",
    "        for a, (start, end) in enumerate(spike_artifact_times):\n",
    "            if start > first_timestamp:\n",
    "                first_artifact = a\n",
    "                break\n",
    "\n",
    "        # plot artifacts interval times\n",
    "        view.add_interval_series(\n",
    "            name='detected artifacts',\n",
    "            t_start=np.asarray(spike_artifact_times, dtype=np.float64)[first_artifact:last_artifact, 0],\n",
    "            t_end=np.asarray(spike_artifact_times, dtype=np.float64)[first_artifact:last_artifact, 1],\n",
    "            color='lightcoral',\n",
    "            border_color='lightcoral',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2d62a",
   "metadata": {},
   "source": [
    "Load in and plot spike times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "000ca0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_time_window = 0.00005  # window to plot around each spike time (in seconds)\n",
    "if plot_spikes:\n",
    "    # load in spike times to confirm that we're picking up the spikes we think we are with the clusterless thresholder\n",
    "    features_param_name = 'amplitude'\n",
    "    sort_interval_name = interval_list_name\n",
    "    sorter_name = 'clusterless_thresholder'\n",
    "    sorting_param_name = 'default_clusterless'\n",
    "\n",
    "    recording_id = (sgs.SpikeSortingRecordingSelection() & {\n",
    "                        'nwb_file_name': nwb_file_name, \n",
    "                        'interval_list_name': sort_interval_name,\n",
    "                        'preproc_param_name': 'default',\n",
    "                        'sort_group_id': sort_group_id,\n",
    "                    }).fetch1('recording_id')\n",
    "\n",
    "    spikesorting_merge_id = ((SpikeSortingOutput.CurationV1 * sgs.SpikeSortingSelection) & {\n",
    "        'nwb_file_name': nwb_file_name,\n",
    "        'recording_id': recording_id,\n",
    "        'sorter': sorter_name,\n",
    "        'sorter_param_name': sorting_param_name,\n",
    "    }).fetch1('merge_id')\n",
    "\n",
    "    waveform_s_key = {\n",
    "            'spikesorting_merge_id': spikesorting_merge_id,\n",
    "            'features_param_name': features_param_name,\n",
    "        }\n",
    "\n",
    "    sort_group_spike_times, spike_waveform_features = (\n",
    "        UnitWaveformFeatures & waveform_s_key\n",
    "    ).fetch_data()\n",
    "\n",
    "    # fetch the spikes times for one particular sort group\n",
    "    sort_group_spike_time_int_starts = sort_group_spike_times[0] - spike_time_window\n",
    "    sort_group_spike_time_int_ends = sort_group_spike_times[0] + spike_time_window\n",
    "    sort_group_spike_time_ints = list(zip(sort_group_spike_time_int_starts, sort_group_spike_time_int_ends))\n",
    "\n",
    "    # add spike times to plot\n",
    "    if len(sort_group_spike_time_ints) == 0:\n",
    "        print(f'No spike time intervals were detected with sorter {sorter_name}')\n",
    "    else:\n",
    "        # convert artifact times to be on the same timescale as the data (zeroed to start of the interval list)\n",
    "        if zero_win_start:\n",
    "            sort_group_spike_time_ints = sort_group_spike_time_ints - win_start_time\n",
    "        else:\n",
    "            sort_group_spike_time_ints = sort_group_spike_time_ints - int_start_time\n",
    "\n",
    "        last_timestamp = timestamps[-1]\n",
    "        last_spike = len(sort_group_spike_time_ints)\n",
    "        for a, (start, end) in enumerate(sort_group_spike_time_ints):\n",
    "            if end > last_timestamp:\n",
    "                last_spike = a\n",
    "                break\n",
    "        \n",
    "        first_timestamp = timestamps[0]\n",
    "        first_spike = 0\n",
    "        for a, (start, end) in enumerate(sort_group_spike_time_ints):\n",
    "            if start > first_timestamp:\n",
    "                first_spike = a\n",
    "                break\n",
    "\n",
    "        # plot spike times interval times\n",
    "        view.add_interval_series(\n",
    "            name='detected spikes',\n",
    "            t_start=np.asarray(sort_group_spike_time_ints, dtype=np.float64)[first_spike:last_spike, 0],\n",
    "            t_end=np.asarray(sort_group_spike_time_ints, dtype=np.float64)[first_spike:last_spike, 1],\n",
    "            color='green',\n",
    "            border_color='green',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd6223",
   "metadata": {},
   "source": [
    "Plot spike-sorted electrode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f320e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot electrodes in a uniform time series\n",
    "view.add_uniform_series(\n",
    "    name='ca1 electrodes',\n",
    "    start_time_sec=timestamps[0],\n",
    "    sampling_frequency_hz=sampling_frequency_hz,\n",
    "    data=full_data[elecs].values,\n",
    "    channel_names=labels,\n",
    "    colors=colors,\n",
    "    width=1,\n",
    "    channel_spacing=elec_separation,\n",
    "    timestamps_for_inserting_nans=timestamps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e265bee6",
   "metadata": {},
   "source": [
    "Generate and show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42ac7755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 files to upload, total size: 3735.76 MB\n",
      "Uploading 46 files in batches of 20 with up to 16 concurrent uploads per batch...\n",
      "Processing batch 1/3 (20 files)...\n",
      "Uploaded 1/46: extension_manifest.json\n",
      "Uploaded 2/46: index.html\n",
      "Uploaded 3/46: data.zarr/_consolidated_22.dat\n",
      "Uploaded 4/46: data.zarr/_consolidated_33.dat\n",
      "Uploaded 5/46: data.zarr/_consolidated_35.dat\n",
      "Uploaded 6/46: data.zarr/_consolidated_1.dat\n",
      "Uploaded 7/46: data.zarr/_consolidated_0.dat\n",
      "Uploaded 8/46: data.zarr/_consolidated_26.dat\n",
      "Uploaded 9/46: data.zarr/_consolidated_16.dat\n",
      "Uploaded 10/46: data.zarr/_consolidated_17.dat\n",
      "Uploaded 11/46: data.zarr/_consolidated_4.dat\n",
      "Uploaded 12/46: data.zarr/_consolidated_25.dat\n",
      "Uploaded 13/46: data.zarr/_consolidated_10.dat\n",
      "Uploaded 14/46: data.zarr/_consolidated_14.dat\n",
      "Uploaded 15/46: data.zarr/_consolidated_28.dat\n",
      "Uploaded 16/46: data.zarr/_consolidated_29.dat\n",
      "Uploaded 17/46: data.zarr/_consolidated_27.dat\n",
      "Uploaded 18/46: data.zarr/_consolidated_3.dat\n",
      "Uploaded 19/46: data.zarr/_consolidated_38.dat\n",
      "Uploaded 20/46: data.zarr/_consolidated_13.dat\n",
      "Processing batch 2/3 (20 files)...\n",
      "Uploaded 21/46: data.zarr/.zmetadata\n",
      "Uploaded 22/46: data.zarr/_consolidated_12.dat\n",
      "Uploaded 23/46: data.zarr/_consolidated_39.dat\n",
      "Uploaded 24/46: data.zarr/_consolidated_18.dat\n",
      "Uploaded 25/46: data.zarr/_consolidated_19.dat\n",
      "Uploaded 26/46: data.zarr/_consolidated_21.dat\n",
      "Uploaded 27/46: data.zarr/_consolidated_32.dat\n",
      "Uploaded 28/46: data.zarr/_consolidated_23.dat\n",
      "Uploaded 29/46: data.zarr/_consolidated_7.dat\n",
      "Uploaded 30/46: data.zarr/_consolidated_37.dat\n",
      "Uploaded 31/46: data.zarr/_consolidated_36.dat\n",
      "Uploaded 32/46: data.zarr/_consolidated_11.dat\n",
      "Uploaded 33/46: data.zarr/_consolidated_20.dat\n",
      "Uploaded 34/46: data.zarr/_consolidated_34.dat\n",
      "Uploaded 35/46: data.zarr/_consolidated_24.dat\n",
      "Uploaded 36/46: data.zarr/_consolidated_2.dat\n",
      "Uploaded 37/46: data.zarr/_consolidated_5.dat\n",
      "Uploaded 38/46: data.zarr/_consolidated_8.dat\n",
      "Uploaded 39/46: data.zarr/_consolidated_6.dat\n",
      "Uploaded 40/46: data.zarr/_consolidated_9.dat\n",
      "Processing batch 3/3 (6 files)...\n",
      "Uploaded 41/46: assets/index-GPjx4QpG.css\n",
      "Uploaded 42/46: assets/neurosift-logo-CLsuwLMO.png\n",
      "Uploaded 43/46: assets/index-BY1Hwjm4.js\n",
      "Uploaded 44/46: data.zarr/_consolidated_30.dat\n",
      "Uploaded 45/46: data.zarr/_consolidated_31.dat\n",
      "Uploaded 46/46: data.zarr/_consolidated_15.dat\n",
      "Creating manifest...\n",
      "Total size: 3735.76 MB\n",
      "Uploading manifest.json...\n",
      "Finalizing figure...\n",
      "Upload completed successfully\n",
      "View the figure at: https://gillespielab.figpack.org/figures/default/d502ce9ab999deedbacad7ad/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://gillespielab.figpack.org/figures/default/d502ce9ab999deedbacad7ad/index.html'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = vv.Box(\n",
    "    title=f\"{nwb_file_name} {interval_list_name}\",\n",
    "    direction='vertical',\n",
    "    items=[\n",
    "        vv.LayoutItem(view, title='ca1 electrodes', stretch=1),\n",
    "    ]\n",
    ")\n",
    "layout.show(title=f\"{nwb_file_name} {interval_list_name} {coinc_interval_list_name if (artifact_type == 'coincident_spikes') else ''}\", upload=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabby_spyglass_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
